{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document contains information regarding the evaluation of the classification model using 3D images. That is, it shows how accurately the 3D image model can classify autism and nonautism test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ukNDsYjllqRT",
    "outputId": "b01ad8a1-cfb8-4fc0-d2ea-ddf53ff9af16"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "\n",
    "from keras.layers import Input,Dense,Flatten,Dropout,Conv3D,MaxPooling3D,Activation\n",
    "from keras.models import Model,Sequential,load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras import activations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "from PIL import Image\n",
    "import nibabel as nib #reading MR images\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nii_files_center(center_folder):\n",
    "    nii_files = glob.glob(os.path.join(center_folder, '**', '*.nii'), recursive=True)\n",
    "    return len(nii_files)\n",
    "\n",
    "main_folder = 'Autism_TestData'  \n",
    "\n",
    "autism_folder = os.path.join(main_folder, 'Autism')\n",
    "nonautism_folder = os.path.join(main_folder, 'NonAutism')\n",
    "\n",
    "autism_centers = [f.path for f in os.scandir(autism_folder) if f.is_dir()]\n",
    "nonautism_centers = [f.path for f in os.scandir(nonautism_folder) if f.is_dir()]\n",
    "\n",
    "autism_total_count = 0\n",
    "nonautism_total_count = 0\n",
    "\n",
    "for center_folder in autism_centers:\n",
    "    nii_count = count_nii_files_center(center_folder)\n",
    "    autism_total_count += nii_count\n",
    "    center_name = os.path.basename(center_folder)\n",
    "    print(f\"Autism - Recruiting Center: {center_name}, Number of .nii files: {nii_count}\")\n",
    "\n",
    "for center_folder in nonautism_centers:\n",
    "    nii_count = count_nii_files_center(center_folder)\n",
    "    nonautism_total_count += nii_count\n",
    "    center_name = os.path.basename(center_folder)\n",
    "    print(f\"Nonautism - Recruiting Center: {center_name}, Number of .nii files: {nii_count}\")\n",
    "\n",
    "print(f\"Total number of .nii files in Autism folder: {autism_total_count}\")\n",
    "print(f\"Total number of .nii files in NonAutism folder: {nonautism_total_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the training phase I extract some slices from each axis so that the dimensions of the images are consistent and I can run the model.\n",
    "The test dataset contains 52 images.\n",
    "- **Autism**: 20 images\n",
    "- **Nonautism**: 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = nib.load(image_path).get_fdata()\n",
    "    \n",
    "    # Extract middle slices along y-axis\n",
    "    img = img[30:110, 60:126, 60:126]\n",
    "        \n",
    "    # Z-score Standardization\n",
    "    mean = img.mean()\n",
    "    std = img.std()\n",
    "    img = (img - mean) / std\n",
    "    \n",
    "    return img\n",
    "\n",
    "def load_images_and_labels(data_folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get the list of .nii files\n",
    "    image_paths = glob.glob(os.path.join(data_folder, '*.nii'))\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        img = preprocess_image(image_path)\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Prepare the data\n",
    "data_root = 'TestDF'  # Replace with your actual root directory\n",
    "autism_folder = os.path.join(data_root, 'Autism')\n",
    "nonautism_folder = os.path.join(data_root, 'NonAutism')\n",
    "\n",
    "autism_images, autism_labels = load_images_and_labels(autism_folder, 1)\n",
    "print(\"Autism Images Length:\", len(autism_images))\n",
    "\n",
    "\n",
    "# Print dimensions of each autism image\n",
    "for i, img in enumerate(autism_images):\n",
    "    print(f\"Autism Image {i + 1} Dimensions: {img.shape}\")\n",
    "\n",
    "# Load and preprocess nonautism images\n",
    "nonautism_images, nonautism_labels = load_images_and_labels(nonautism_folder, 0)\n",
    "print(\"Nonautism Images Length:\", len(nonautism_images))\n",
    "\n",
    "# Print dimensions of each nonautism image\n",
    "for i, img in enumerate(nonautism_images):\n",
    "    print(f\"Nonautism Image {i + 1} Dimensions: {img.shape}\")\n",
    "\n",
    "\n",
    "# Combine the data and labels\n",
    "images = np.concatenate((autism_images, nonautism_images), axis=0)\n",
    "labels = autism_labels + nonautism_labels\n",
    "\n",
    "# Convert to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "labels = to_categorical(labels)  # Convert labels to one-hot encoded format\n",
    "\n",
    "\n",
    "print(\"Images Shape:\", images.shape)\n",
    "print(\"Labels Shape:\", labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model's classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = nib.load(image_path).get_fdata()\n",
    "    \n",
    "    # Extract middle slices along y-axis\n",
    "    img = img[30:110, 60:126, 60:126]\n",
    "        \n",
    "    # Z-score Standardization\n",
    "    mean = img.mean()\n",
    "    std = img.std()\n",
    "    img = (img - mean) / std\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Define the load_images_and_labels function\n",
    "def load_images_and_labels(data_folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get the list of .nii files\n",
    "    nii_files = glob.glob(os.path.join(data_folder, '*.nii'))\n",
    "    \n",
    "    for image_path in nii_files:\n",
    "        img = preprocess_image(image_path)\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('Autism_model.h5')  # Replace with the actual path to your model\n",
    "\n",
    "# Load and preprocess the images\n",
    "data_root = 'TestDF'  # Replace with your actual root directory\n",
    "autism_folder = os.path.join(data_root, 'Autism')\n",
    "nonautism_folder = os.path.join(data_root, 'NonAutism')\n",
    "\n",
    "autism_images, autism_labels = load_images_and_labels(autism_folder, 1)\n",
    "nonautism_images, nonautism_labels = load_images_and_labels(nonautism_folder, 0)\n",
    "\n",
    "# Combine the data and labels\n",
    "images = np.concatenate((autism_images, nonautism_images), axis=0)\n",
    "labels = np.concatenate((autism_labels, nonautism_labels), axis=0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "labels = to_categorical(labels)  # Convert labels to one-hot encoded format\n",
    "\n",
    "# Make predictions using the model\n",
    "predictions = model.predict(images)\n",
    "\n",
    "# Convert predictions to class labels (0 or 1)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_labels == labels.argmax(axis=1)) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Print actual and predicted classifications\n",
    "for i in range(len(images)):\n",
    "    actual_classification = 'Autism' if labels[i][1] == 1 else 'NonAutism'\n",
    "    predicted_classification = 'Autism' if predicted_labels[i] == 1 else 'NonAutism'\n",
    "    \n",
    "    print(f\"Image {i+1} - Actual: {actual_classification} - Predicted: {predicted_classification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(labels.argmax(axis=1), predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"OrRd\", xticklabels=[\"NonAutism\", \"Autism\"], yticklabels=[\"NonAutism\", \"Autism\"])\n",
    "plt.xlabel(\"Predicted\",  fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"Actual\",  fontsize=14, fontweight='bold')\n",
    "plt.title(\"Confusion Matrix\",  fontsize=14, fontweight='bold')\n",
    "plt.savefig(\"Autism_Confusion Matrix.png\", dpi = 300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(labels.argmax(axis=1), predicted_labels)\n",
    "precision = precision_score(labels.argmax(axis=1), predicted_labels)\n",
    "f1 = f1_score(labels.argmax(axis=1), predicted_labels)\n",
    "\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy: 63.46%**\n",
    "    \n",
    "**Recall: 10%**\n",
    "    \n",
    "**Precision: 67%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of limitations which might have contributed to the not high accuracy of the classification model:\n",
    " 1. **Computational resources and hardware capacity**:\n",
    " Running a training model with 3D images require high speed computers with big capacity so that it contributes to the processing time.\n",
    "\n",
    " 2. **Sample size**:\n",
    " As mentioned in the other document, due to the above reasons, I had to minimize the number of images used for the training model. However, more sample size can provide more amount of information and therefore can contribute to better classification and accuracy. Thus, more sample size can be used if there is more computational power.\n",
    "\n",
    " 3. **Model complexity**:\n",
    " The majority of the models and features are designed for 2D image classification which makes the 3D model training model complex.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3DKerasAttempt.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
